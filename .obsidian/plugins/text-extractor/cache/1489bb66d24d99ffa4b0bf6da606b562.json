{"path":"attachments/Deep Reinforcement Learning-22.png","text":"Policy Gradient * Pros vs DQN: If Q function is too complex to be learned, DQN may fail miserably, while PG will still learn a good policy. ‘ I: Faster convergence Capable of learning stochastic policies - DQN can’t , Much easier to model continuous action space T * Cons vs DQN: ] ‘«,‘ | ' Sample inefficient (needs more data) \\ Less stable during training process. F * Poor to (state, action) pairs for delayed rewards e - — = AWESOME GOOD GOOD GOOD R A A, A; Ay A, Calculating the reward at the . . \\ end, means all the actions will be averaged as good because Goop GooD Goop GooD GooD the total reward was high. % % % % T % A, A, As As A, [T e forthe Ml st of reforences v [63, 335] https://deeplearning.mit.edu 2019","libVersion":"0.3.2","langs":"eng"}