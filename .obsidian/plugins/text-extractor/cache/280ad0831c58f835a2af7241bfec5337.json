{"path":"attachments/Deep Reinforcement Learning-8.png","text":"MIT 6.S091: Introduction to Deep Reinforcement Learning (Deep RL c » Q-Learning ] a /?\\ State-action value function: Q*(s,a) . * Expected return when starting in s, /& l\\ l\\ performing a, and following & g’ * Q-Learning: Use any policy to estimate Q that maximizes future reward: ) - <h = * Qdirectly approximates Q* (Bellman optimality equation) A . ‘ * Independent of the policy being followed “ * Only requirement: keep updating each (s,a) pair — ! (@\\ e Qt 1 1(st, ar) = Qt(st, ar)+a (Rt+1 + 7y max Qt(str1,a) — Qt(st, at)) .om e","libVersion":"0.3.2","langs":"eng"}